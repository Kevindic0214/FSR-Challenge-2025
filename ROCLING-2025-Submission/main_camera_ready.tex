%
% ROCLING 2025 – Camera-Ready Version (English)
%

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{rocling2025}
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,breaklines=true,breakatwhitespace=true,columns=fullflexible}
\renewcommand{\UrlFont}{\ttfamily\small}

% Camera-ready: uncomment to remove ruler and set final formatting
\roclingfinalcopy

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{A Compact Whisper+LoRA Baseline for Taiwanese Hakka ASR in FSR-2025}

\author{Hung-Ting Hsieh\\
  Independent Researcher\\
  \texttt{hsiehk0214@gmail.com}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a compact baseline for the Formosa Speech Recognition (FSR-2025) Taiwanese Hakka ASR challenge.\
Our system fine-tunes \emph{Whisper-large-v2} \citep{radford2022whisper} with LoRA \citep{hu2021lora}, using consistent text normalization and balanced dev splits.\
On the official warm-up set, we obtain 10.94\% CER for Track~1 (Hanzi) and 28.48\% SER for Track~2 (Pinyin).\
We provide simple yet reproducible pipelines covering data preparation, training, inference, and evaluation.\
Code is available at \href{https://github.com/Kevindic0214/FSR-Challenge-2025}{github.com/Kevindic0214/FSR-Challenge-2025}.
\end{abstract}

\begin{keywords}
Automatic speech recognition; Hakka; Whisper; LoRA; low-resource; FSR-2025; CER; SER
\end{keywords}

\section{Introduction}
Taiwanese Hakka is a low-resource language variant of significant cultural value.\
FSR-2025 defines two tracks: Track~1 evaluates character error rate (CER) on Hanzi, and Track~2 evaluates syllable error rate (SER) on Pinyin.\
We aim to provide a strong, minimal-requirement baseline using \emph{Whisper-large-v2} fine-tuned with low-rank adaptation (LoRA), emphasizing practical engineering choices and reproducibility over model complexity.\\
In this work, we follow the official specification of the FSR-2025 challenge \citep{fsr2025}.

\paragraph{Contributions.} (i) A reproducible baseline for both tracks with unified normalization that matches evaluation; (ii) a simple LoRA recipe runnable on a single 24~GB GPU with balanced speaker-based dev split; (iii) competitive warm-up results with lightweight error and length-bucket analyses.

\section{Task and Data}
We train on the HAT-Vol2 corpus (\textasciitilde60 hours; Dapu and Zhao'an dialects; 16~kHz mono) and evaluate on the FSR-2025 warm-up set (\textasciitilde10 hours; 4{,}299 utterances total).\
We build manifests via dedicated scripts for each track, apply Unicode NFKC normalization, remove zero-width characters, and adopt track-specific text processing: Hanzi cleaning for Track~1 and Pinyin digit-tone policy for Track~2.\
Dev speakers are selected in a balanced way across DF/DM/ZF/ZM groups for stable validation.\\
We rely on the HAT-Vol2 dataset \citep{hatvol2} and the official warm-up set \citep{fsr2025} for training and evaluation.

\paragraph{Normalization policy.} Track~1 (Hanzi): apply NFKC, remove zero-width characters, map mixed punctuation to Chinese forms, and strip spaces to align with evaluation. Track~2 (Pinyin): apply NFKC, remove zero-width characters, map ü/ǘ/\dots{} and ``u:/U:'' to ``v'', keep only [a--z0--9] and single spaces, and by default drop starred syllables (e.g., ``*ki53'' or ``ki53*''); an optional fix merges split-tone forms (e.g., ``ki 53'' $\rightarrow$ ``ki53'').

\begin{table}[h]
\centering
\begin{tabular}{lcl}
\hline
\textbf{Split} & \textbf{Size} & \textbf{Notes} \\
\hline
HAT-Vol2 (train) & \(\sim\)60 h & Dapu/Zhao'an; 16 kHz mono \\
Warm-up (eval) & \(\sim\)10 h / 4,299 utt & Official FSR-2025 set \\
Dev speakers & 12 (balanced) & DF/DM/ZF/ZM allocation \\
\hline
\end{tabular}
\caption{Dataset overview and evaluation split.}
\label{tab:data}
\end{table}

\section{Related Work}
Low-resource ASR has been explored in multilingual programs such as Babel \citep{harper2014babel}. Whisper \citep{radford2022whisper} is a strong multilingual recognizer; we adapt it to Hakka via parameter-efficient fine-tuning. LoRA \citep{hu2021lora} reduces trainable parameters for seq2seq models while retaining quality, enabling practical fine-tuning on 24~GB GPUs.

\section{Approach}
We fine-tune \emph{Whisper-large-v2} \citep{radford2022whisper} with LoRA \citep{hu2021lora} (rank~16, $\alpha$=32, dropout~0.05).\
Training uses gradient checkpointing, bf16 when available, and label smoothing.\
For Track~1 decoding, we force Chinese transcription via the decoder prompt; Track~2 uses language-appropriate decoding without language forcing.\
Beam search with 5 beams and temperature 0.0 is used unless specified.\\
Implementation details: we apply LoRA adapters to attention and MLP modules (\texttt{q\_proj}, \texttt{k\_proj}, \texttt{v\_proj}, \texttt{out\_proj}, \texttt{fc1}, \texttt{fc2}); enable TF32 for faster, stable training on recent GPUs; and use label smoothing of 0.1.

We keep Whisper's default suppression behavior (do not forcibly clear \texttt{suppress\_tokens}), disable the generation cache during training, and enable early stopping on the dev metric (patience~2). bf16 is automatically used when supported; otherwise fp16 on GPU.

\paragraph{Implementation.} We implement training and inference with HuggingFace Transformers and PEFT on PyTorch. Audio I/O uses \texttt{torchaudio} for Track~1 and \texttt{soundfile}+\texttt{librosa} for Track~2. Manifests are JSONL with fields \{\texttt{utt\_id}, \texttt{audio}, \texttt{text}/\texttt{hanzi}/\texttt{pinyin}, \texttt{group}\}; relative audio paths are resolved via a root flag. During training we enable gradient checkpointing (non-reentrant when available), set \texttt{use\_cache}=False, and turn on TF32. Decoding uses \texttt{num\_beams}=5, \texttt{temperature}=0.0, \texttt{no\_repeat\_ngram\_size}=3, \texttt{length\_penalty}=1.0, and \texttt{max\_new\_tokens}=256; we force a Chinese decoder prompt only for Track~1. We log CER/SER, exact match, group/length-bucket scores, 3-gram repetition rate, throughput, and peak memory; training uses AdamW with a linear schedule and 500 warmup steps, and we save only the LoRA adapter and processor for lightweight deployment.

\section{Experiments}
We train for 3 epochs with per-device batch size 2 and gradient accumulation 16 on an RTX~4090 (24~GB).\
Evaluation metrics are CER (Track~1) and SER (Track~2) with sentence-level exact match for reference. We use seed 1337, learning rate $1\times 10^{-4}$ with 500 warmup steps, label smoothing 0.1, gradient checkpointing, TF32, and early stopping (patience~2). The HuggingFace Trainer default optimizer (AdamW) is used.

\section{Results}
On the warm-up set: Track~1 reaches 10.94\% CER with 58.06\% exact match; Track~2 reaches 28.48\% SER with 12.17\% exact match.\
These numbers are obtained with the shared pipelines and no external data beyond the provided corpora.\
We observe stable validation under balanced speaker splits and consistent normalization. Longer utterances show mildly higher error rates (notably in the 12.4--20~s bucket), and we observe small variations across DF/DM/ZF/ZM groups under the balanced-split protocol.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Track} & \textbf{Metric} & \textbf{Score} \\
\hline
Track 1 (Hanzi) & CER / EM & 10.94\% / 58.06\% \\
Track 2 (Pinyin) & SER / EM & 28.48\% / 12.17\% \\
\hline
\end{tabular}
\caption{Warm-up evaluation results. EM: exact match.}
\label{tab:results}
\end{table}

\paragraph{Final-test results.} On the official final-test, our system achieves 18.78\% CER for Track~1 (ranked 2/3 in our social group) and 33.38\% SER for Track~2. For analysis, we also report a tone-removed Pinyin WER of 21.30\% (ranked 2/2 among teams with available data). Figure~\ref{fig:finaltest} shows the official charts.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Set} & \textbf{Track} & \textbf{Metric} & \textbf{Score (Rank)} \\
\hline
Final-test & 1 (Hanzi) & CER & 18.78\% (2/3) \\
Final-test & 2 (Pinyin) & SER & 33.38\% (2/2*) \\
Final-test & 2 (Pinyin) & WER (tone-removed) & 21.30\% (2/2*) \\
\hline
\end{tabular}
\caption{Official final-test results. *Among teams with available data in our social group.}
\label{tab:final}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\linewidth]{final-test-results/c051e193-1390-464d-b515-e1b62d05aa9f.png}\\[0.3em]
\includegraphics[width=0.98\linewidth]{final-test-results/dc5b29d9-c132-4733-bab4-7ef8b2af69fd.png}
\caption{Challenge 2025 final-test result charts.}
\label{fig:finaltest}
\end{figure}

\section{Reproducibility}
Code and scripts are available at \href{https://github.com/Kevindic0214/FSR-Challenge-2025}{github.com/Kevindic0214/FSR-Challenge-2025}.\\
We provide end-to-end scripts for data preparation, training, inference, and evaluation. Minimal examples and notes:\\
\textit{Notes.} Track~1 defaults keep asterisks and punctuation unless \texttt{--strip\_asterisk}/\texttt{--strip\_punct} is specified. Track~2 drops starred syllables by default and supports optional split-tone merging with \texttt{--fix\_split\_tone}. All runs use seed 1337.\\
\textbf{Track 1}:\
\begin{lstlisting}
python prepare_hakka_track1.py --root HAT-Vol2 \
  --drop_mispronounce --relative_audio_path
python train_whisper_lora_track1.py \
  --train_jsonl HAT-Vol2/manifests_track1/train.jsonl \
  --dev_jsonl   HAT-Vol2/manifests_track1/dev.jsonl
python infer_track1.py --eval_root FSR-2025-Hakka-evaluation \
  --outfile predictions_track1.csv --model openai/whisper-large-v2 \
  --lora_dir runs/track1/lora_v2_r16_e3
python eval_track1_cer.py --key_dir FSR-2025-Hakka-evaluation-key \
  --hyp predictions_track1.csv
\end{lstlisting}
\textbf{Track 2}:\
\begin{lstlisting}
python prepare_hakka_track2.py --data_root HAT-Vol2 \
  --out_dir HAT-Vol2/manifests_track2 --exclude_mispronounced
python train_whisper_lora_track2.py
python infer_track2.py --eval_root FSR-2025-Hakka-evaluation \
  --outfile predictions_track2.csv --model openai/whisper-large-v2 \
  --lora_dir exp_track2_whisper_large_lora
python eval_track2_ser.py --key_dir FSR-2025-Hakka-evaluation-key \
  --hyp predictions_track2.csv
\end{lstlisting}

\section{Error Analysis}
Common errors include character/phonetic substitutions and occasional short repeats; we monitor n-gram repetition to detect degeneration.\
Performance degrades mildly for longer utterances; bucketed analysis suggests length-aware decoding or better segmenting could help.

\paragraph{Examples.} Sampled warm-up mismatches:\\
\emph{(003jh5p8hd.wav)} ref: 大家攏無仰子\textbf{嗬}隨拁你人救出去; hyp: 大家攏無仰子\textbf{項}隨拁你研究出去.\\
\emph{(03qw9gfad7.wav)} ref: 食\textbf{著}幾隻草蜢\textbf{乜好啊}; hyp: 食\textbf{到佢}隻草蜢\textbf{毋會好啊}.\\
\emph{(04qied7gz8.wav)} ref: \dots 這兜\textbf{地動無幾著呢}莊頭\dots; hyp: \dots 這兜\textbf{地圖無幾臭呢啊}莊頭\dots

These illustrate homophone/near-neighbor substitutions and local phrase alterations; stronger language modeling or constrained decoding may mitigate such errors. For Pinyin (Track~2), common patterns include tone-digit confusions and occasional effects from star-syllable handling; our normalization reduces such artifacts.

\section{Conclusion}
We provide a concise, reproducible baseline for both tracks of FSR-2025 Hakka ASR using Whisper+LoRA.\
Future work includes dialect-aware adaptation, LM-rescoring for Hanzi, refined Pinyin normalization, and temperature/beam tuning.

\section*{Limitations}
Our results are based on the provided HAT-Vol2 training data and the official warm-up set. We do not explore external language models or data augmentation; Pinyin normalization choices (e.g., starred syllables) can affect SER.

\section*{Acknowledgments}
We thank the FSR-2025 organizers and dataset providers.

\bibliographystyle{acl_natbib}
\bibliography{refs}

\end{document}
