# FSR-2025 Hakka ASR â€“ Progress Snapshot (2025-08-23)

**Owner:** Kevin (NYCU)
**GPU:** RTX 4090 24GB (CUDA 12.6)
**Goal:** Finish Pilot submission for both tracks with a reproducible, extensible pipeline.

---

## 0) TL;DR

* **Track 2 (Pinyin) Pilot score:** **SER = 25.69%** on warm-up eval (4299 utts).

  * Reference tokens = **71,354**; Total edits = **18,332**; Sentence exact-match = **18.42%**.
  * Treating `*token` as literal slightly worsens score (**SER 25.72%**). â†’ **Decision:** drop `*token` in reference for scoring.
  * Cleaned submission **Level-Up\_pinyin\_keyonly.csv** (kept=4299, dropped extras=105). Header added; no missing IDs.
* **Track 1 (Hanzi):** LoRA training done (training loss \~0.028). Eval CER & warm-up inference pending.

**Next 3 steps (priority):**

1. **Run Track1 inference** â†’ produce `Level-Up_hanzi.csv`.
2. **Add Track1 evaluation** (CER) via `eval_track1_cer.py` (mirror Track2 eval but char-level).
3. **Pilot-ready packaging** â†’ `NYCU_Level-Up_pinyin.csv` + `NYCU_Level-Up_hanzi.csv` (format & headers verified).

---

## 1) Repository / Workspace status

```
FSR-Challenge-2025/
â”œâ”€â”€ data & keys
â”‚   â”œâ”€â”€ HAT-Vol2/
â”‚   â”œâ”€â”€ FSR-2025-Hakka-evaluation/
â”‚   â””â”€â”€ FSR-2025-Hakka-evaluation-key/
â”‚
â”œâ”€â”€ preparation
â”‚   â”œâ”€â”€ prepare_hakka_track2.py      # build manifests (pinyin)
â”‚   â””â”€â”€ prepare_hakka_track1.py      # build manifests (hanzi)
â”‚
â”œâ”€â”€ training
â”‚   â”œâ”€â”€ train_whisper_lora_track2.py # Whisper-large-v2 + LoRA (pinyin)
â”‚   â””â”€â”€ train_whisper_lora_track1.py # Whisper-large-v2 + LoRA (hanzi)
â”‚
â”œâ”€â”€ inference
â”‚   â”œâ”€â”€ infer_whisper_track2.py
â”‚   â”œâ”€â”€ infer_hakka_pinyin_warmup.py # produces Level-Up_pinyin.csv
â”‚   â””â”€â”€ infer_hakka_hanzi_warmup.py  # produces Level-Up_hanzi.csv (ready)
â”‚
â”œâ”€â”€ evaluation
â”‚   â”œâ”€â”€ eval_track2_ser.py           # syllable-level WER (SER) with key-dir ingestion
â”‚   â”œâ”€â”€ make_keyonly_track2.py       # filter predictions to key IDs only
â”‚   â”œâ”€â”€ quick_ser_check.py           # dev quick check for Track2
â”‚   â””â”€â”€ plot_loss.py                 # plots from trainer_state.json
â”‚
â”œâ”€â”€ experiments
â”‚   â”œâ”€â”€ exp_track2_whisper_large_lora/
â”‚   â””â”€â”€ exp_track1_whisper_large_lora/
â”‚
â”œâ”€â”€ submissions / logs
â”‚   â”œâ”€â”€ Level-Up_pinyin.csv          # raw (had +105 extra)
â”‚   â”œâ”€â”€ Level-Up_pinyin_keyonly.csv  # clean (4299 rows, header added)
â”‚   â”œâ”€â”€ aligned_pinyin.csv           # per-utt SER details (drop *token)
â”‚   â”œâ”€â”€ aligned_pinyin_keepstar.csv  # per-utt SER details (keep *token)
â”‚   â””â”€â”€ decode_track2.tsv
â””â”€â”€ docs
    â”œâ”€â”€ README.md
    â””â”€â”€ so_far.md (older notes)
```

---

## 2) Current metrics & decisions

### Track 2 â€“ Warm-up evaluation

* **Command:**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv
  ```
* **Result:**

  * UTT = **4299**
  * REF\_TOKENS = **71,354**
  * TOTAL\_EDITS = **18,332**
  * **SER = 0.2569** (25.69%)
  * Sentence exact-match rate = **18.42%**
  * No warnings (IDs aligned).
* **Star-token sensitivity:**

  * Keep `*token` â†’ SER **0.2572** (slightly worse; REF\_TOKENS=71,386; edits=18,361).
  * **Decision:** **drop `*token`** from reference for SER (represents coalesced syllables not actually spoken).

### Track 1 â€“ Training snapshot

* Whisper-large-v2 + LoRA trained to **loss \~0.028**.
* Missing: formal dev CER; warm-up inference to produce `Level-Up_hanzi.csv`.

---

## 3) Normalization & scoring policies (locked-in)

**Track 2 (Pinyin / SER):**

* Tokenization = whitespace-separated syllables.
* Normalize both sides: lowercase; keep only `[a-z0-9]`; collapse multiple spaces â†’ single space.
* **Reference:** drop tokens starting with `*` (åˆéŸ³æ¨™è¨˜)ã€‚
* **Metric:** SER = sum(edit distance over tokens) / sum(#ref tokens).

**Track 1 (Hanzi / CER):**

* Remove spaces; remove `*` before scoring.
* Metric: CER = sum(edit distance over characters) / sum(#ref chars).

---

## 4) Whatâ€™s done vs outstanding

### Done âœ…

* Track2 LoRA training & warm-up inference â†’ `Level-Up_pinyin.csv`ã€‚
* Clean filtering to key set â†’ `Level-Up_pinyin_keyonly.csv` (kept=4299, extras removed=105, header added)ã€‚
* Official-key ingestion + robust SER evaluator â†’ `eval_track2_ser.py`ã€‚
* Per-utt details logged â†’ `aligned_pinyin.csv`ï¼ˆå¯ç”¨æ–¼èª¤å·®åˆ†æžï¼‰ã€‚

### Outstanding ðŸš§

1. Track1 warm-up inference â†’ produce `Level-Up_hanzi.csv`ã€‚
2. Track1 evaluator (`eval_track1_cer.py`) with the same robust IO + normalizationã€‚
3. Optional: per-bucket SER breakdown (DF/DM/ZF/ZM/media) & Top-k worst utts to guide fixesã€‚

---

## 5) Near-term priorities (P0 â†’ P2)

**P0 â€“ Pilot completeness (no retraining):**

1. **Track1 inference** with `infer_hakka_hanzi_warmup.py` â†’ produce `Level-Up_hanzi.csv`ã€‚
2. **Track1 CER eval** using a new `eval_track1_cer.py` (mirror of Track2, char-level)ã€‚
3. **Submission packaging**: rename files to `NYCU_Level-Up_pinyin.csv` & `NYCU_Level-Up_hanzi.csv` (UTF-8, header present)ã€‚

**P1 â€“ Low-cost score improvements (decode-side):**

* Beam sweep (`beams={1,5,8}`) + set `max_new_tokens â‰¥ 225` to reduce deletions on long utterancesã€‚
* (Optional) **KenLM 5-gram rescoring** on N-best for Track2 (typical gain: 0.2â€“0.6 abs%).

**P2 â€“ Data/model adjustments (if diagnosis demands):**

* If deletions dominate: segmentation/VAD or longer decode; tune length penaltyã€‚
* If substitutions dominate: LM rescoring; small LoRA refresh with light augmentation (SpecAugment, speed perturb)ã€‚
* If media domain is worse: domain-targeted augmentation and/or conditional prompts (dialect tags: DF/DM/ZF/ZM)ã€‚

---

## 6) Useful one-liners

* **Filter predictions to key IDs only (done):**

  ```bash
  python make_keyonly_track2.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin.csv \
    --out Level-Up_pinyin_keyonly.csv
  ```
* **Evaluate Track2 SER (drop `*token` by default):**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv
  ```
* **Compare with keeping `*token` (sanity check):**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv \
    --keep_star_tokens \
    --aligned_out aligned_pinyin_keepstar.csv
  ```

---

## 7) Submission checklist (Pilot)

* [ ] `NYCU_Level-Up_pinyin.csv` (UTF-8; header: `éŒ„éŸ³æª”æª”å,è¾¨èªçµæžœ`; only key IDs; normalized; no `*`)
* [ ] `NYCU_Level-Up_hanzi.csv` (UTF-8; header: `éŒ„éŸ³æª”æª”å,è¾¨èªçµæžœ`; only key IDs; `*` removed)
* [ ] System description form (brief model specs; Whisper-large-v2 + LoRA; features; training recipe)
* [ ] Likelihoods/logprobs if requested (avg logprob per utt; already supported in `infer_whisper_track2.py`)

---

## 8) Notes & risks

* **Rule compliance:** no human listening to eval/test; only decode-side tweaks and generic augmentations.
* **Star-token handling:** consistent policy locked (Track2 drop in ref).
* **Reproducibility:** scripts & seeds fixed; add `report_to=tensorboard` if not already for runs.

---

## 9) Next actions (owner â†’ Kevin)

* [ ] Run `infer_hakka_hanzi_warmup.py` â†’ produce `Level-Up_hanzi.csv`ã€‚
* [ ] Implement `eval_track1_cer.py` and score warm-up Hanziã€‚
* [ ] Beam sweep for Track2 (1/5/8) and pick best for Pilot submissionã€‚
* [ ] Optional: bucket SER analysis and top-25 worst utterances for error taxonomyã€‚

> This doc captures the state as of **2025-08-23** and should be updated after Track1 scoring or any decode/LM changes.
