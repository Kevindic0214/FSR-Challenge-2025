# FSR-2025 Hakka ASR – Progress Snapshot (2025-08-23)

**Owner:** Kevin (NYCU)
**GPU:** RTX 4090 24GB (CUDA 12.6)
**Goal:** Finish Pilot submission for both tracks with a reproducible, extensible pipeline.

---

## 0) TL;DR

* **Track 2 (Pinyin) Pilot score:** **SER = 25.69%** on warm-up eval (4299 utts).

  * Reference tokens = **71,354**; Total edits = **18,332**; Sentence exact-match = **18.42%**.
  * Treating `*token` as literal slightly worsens score (**SER 25.72%**). → **Decision:** drop `*token` in reference for scoring.
  * Cleaned submission **Level-Up\_pinyin\_keyonly.csv** (kept=4299, dropped extras=105). Header added; no missing IDs.
* **Track 1 (Hanzi):** LoRA training done (training loss \~0.028). Eval CER & warm-up inference pending.

**Next 3 steps (priority):**

1. **Run Track1 inference** → produce `Level-Up_hanzi.csv`.
2. **Add Track1 evaluation** (CER) via `eval_track1_cer.py` (mirror Track2 eval but char-level).
3. **Pilot-ready packaging** → `NYCU_Level-Up_pinyin.csv` + `NYCU_Level-Up_hanzi.csv` (format & headers verified).

---

## 1) Repository / Workspace status

```
FSR-Challenge-2025/
├── data & keys
│   ├── HAT-Vol2/
│   ├── FSR-2025-Hakka-evaluation/
│   └── FSR-2025-Hakka-evaluation-key/
│
├── preparation
│   ├── prepare_hakka_track2.py      # build manifests (pinyin)
│   └── prepare_hakka_track1.py      # build manifests (hanzi)
│
├── training
│   ├── train_whisper_lora_track2.py # Whisper-large-v2 + LoRA (pinyin)
│   └── train_whisper_lora_track1.py # Whisper-large-v2 + LoRA (hanzi)
│
├── inference
│   ├── infer_whisper_track2.py
│   ├── infer_hakka_pinyin_warmup.py # produces Level-Up_pinyin.csv
│   └── infer_hakka_hanzi_warmup.py  # produces Level-Up_hanzi.csv (ready)
│
├── evaluation
│   ├── eval_track2_ser.py           # syllable-level WER (SER) with key-dir ingestion
│   ├── make_keyonly_track2.py       # filter predictions to key IDs only
│   ├── quick_ser_check.py           # dev quick check for Track2
│   └── plot_loss.py                 # plots from trainer_state.json
│
├── experiments
│   ├── exp_track2_whisper_large_lora/
│   └── exp_track1_whisper_large_lora/
│
├── submissions / logs
│   ├── Level-Up_pinyin.csv          # raw (had +105 extra)
│   ├── Level-Up_pinyin_keyonly.csv  # clean (4299 rows, header added)
│   ├── aligned_pinyin.csv           # per-utt SER details (drop *token)
│   ├── aligned_pinyin_keepstar.csv  # per-utt SER details (keep *token)
│   └── decode_track2.tsv
└── docs
    ├── README.md
    └── so_far.md (older notes)
```

---

## 2) Current metrics & decisions

### Track 2 – Warm-up evaluation

* **Command:**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv
  ```
* **Result:**

  * UTT = **4299**
  * REF\_TOKENS = **71,354**
  * TOTAL\_EDITS = **18,332**
  * **SER = 0.2569** (25.69%)
  * Sentence exact-match rate = **18.42%**
  * No warnings (IDs aligned).
* **Star-token sensitivity:**

  * Keep `*token` → SER **0.2572** (slightly worse; REF\_TOKENS=71,386; edits=18,361).
  * **Decision:** **drop `*token`** from reference for SER (represents coalesced syllables not actually spoken).

### Track 1 – Training snapshot

* Whisper-large-v2 + LoRA trained to **loss \~0.028**.
* Missing: formal dev CER; warm-up inference to produce `Level-Up_hanzi.csv`.

---

## 3) Normalization & scoring policies (locked-in)

**Track 2 (Pinyin / SER):**

* Tokenization = whitespace-separated syllables.
* Normalize both sides: lowercase; keep only `[a-z0-9]`; collapse multiple spaces → single space.
* **Reference:** drop tokens starting with `*` (合音標記)。
* **Metric:** SER = sum(edit distance over tokens) / sum(#ref tokens).

**Track 1 (Hanzi / CER):**

* Remove spaces; remove `*` before scoring.
* Metric: CER = sum(edit distance over characters) / sum(#ref chars).

---

## 4) What’s done vs outstanding

### Done ✅

* Track2 LoRA training & warm-up inference → `Level-Up_pinyin.csv`。
* Clean filtering to key set → `Level-Up_pinyin_keyonly.csv` (kept=4299, extras removed=105, header added)。
* Official-key ingestion + robust SER evaluator → `eval_track2_ser.py`。
* Per-utt details logged → `aligned_pinyin.csv`（可用於誤差分析）。

### Outstanding 🚧

1. Track1 warm-up inference → produce `Level-Up_hanzi.csv`。
2. Track1 evaluator (`eval_track1_cer.py`) with the same robust IO + normalization。
3. Optional: per-bucket SER breakdown (DF/DM/ZF/ZM/media) & Top-k worst utts to guide fixes。

---

## 5) Near-term priorities (P0 → P2)

**P0 – Pilot completeness (no retraining):**

1. **Track1 inference** with `infer_hakka_hanzi_warmup.py` → produce `Level-Up_hanzi.csv`。
2. **Track1 CER eval** using a new `eval_track1_cer.py` (mirror of Track2, char-level)。
3. **Submission packaging**: rename files to `NYCU_Level-Up_pinyin.csv` & `NYCU_Level-Up_hanzi.csv` (UTF-8, header present)。

**P1 – Low-cost score improvements (decode-side):**

* Beam sweep (`beams={1,5,8}`) + set `max_new_tokens ≥ 225` to reduce deletions on long utterances。
* (Optional) **KenLM 5-gram rescoring** on N-best for Track2 (typical gain: 0.2–0.6 abs%).

**P2 – Data/model adjustments (if diagnosis demands):**

* If deletions dominate: segmentation/VAD or longer decode; tune length penalty。
* If substitutions dominate: LM rescoring; small LoRA refresh with light augmentation (SpecAugment, speed perturb)。
* If media domain is worse: domain-targeted augmentation and/or conditional prompts (dialect tags: DF/DM/ZF/ZM)。

---

## 6) Useful one-liners

* **Filter predictions to key IDs only (done):**

  ```bash
  python make_keyonly_track2.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin.csv \
    --out Level-Up_pinyin_keyonly.csv
  ```
* **Evaluate Track2 SER (drop `*token` by default):**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv
  ```
* **Compare with keeping `*token` (sanity check):**

  ```bash
  python eval_track2_ser.py \
    --key_dir FSR-2025-Hakka-evaluation-key \
    --pred_csv Level-Up_pinyin_keyonly.csv \
    --keep_star_tokens \
    --aligned_out aligned_pinyin_keepstar.csv
  ```

---

## 7) Submission checklist (Pilot)

* [ ] `NYCU_Level-Up_pinyin.csv` (UTF-8; header: `錄音檔檔名,辨認結果`; only key IDs; normalized; no `*`)
* [ ] `NYCU_Level-Up_hanzi.csv` (UTF-8; header: `錄音檔檔名,辨認結果`; only key IDs; `*` removed)
* [ ] System description form (brief model specs; Whisper-large-v2 + LoRA; features; training recipe)
* [ ] Likelihoods/logprobs if requested (avg logprob per utt; already supported in `infer_whisper_track2.py`)

---

## 8) Notes & risks

* **Rule compliance:** no human listening to eval/test; only decode-side tweaks and generic augmentations.
* **Star-token handling:** consistent policy locked (Track2 drop in ref).
* **Reproducibility:** scripts & seeds fixed; add `report_to=tensorboard` if not already for runs.

---

## 9) Next actions (owner → Kevin)

* [ ] Run `infer_hakka_hanzi_warmup.py` → produce `Level-Up_hanzi.csv`。
* [ ] Implement `eval_track1_cer.py` and score warm-up Hanzi。
* [ ] Beam sweep for Track2 (1/5/8) and pick best for Pilot submission。
* [ ] Optional: bucket SER analysis and top-25 worst utterances for error taxonomy。

> This doc captures the state as of **2025-08-23** and should be updated after Track1 scoring or any decode/LM changes.
